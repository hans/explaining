---
title             : "The title"
shorttitle        : "Title"

author: 
  - name          : "First Author"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Postal address"
    email         : "my@email.com"
    role:         # Contributorship roles (e.g., CRediT, https://casrai.org/credit/)
      - Conceptualization
      - Writing - Original Draft Preparation
      - Writing - Review & Editing
  - name          : "Ernst-August Doelle"
    affiliation   : "1,2"
    role:
      - Writing - Review & Editing

affiliation:
  - id            : "1"
    institution   : "Wilhelm-Wundt-University"
  - id            : "2"
    institution   : "Konstanz Business School"

authornote: |
  Add complete departmental affiliations for each author here. Each new line herein must be indented, like this line.

  Enter author note here.

abstract: |
  Abstract belongs here.
  
keywords          : "keywords"
wordcount         : "X"

bibliography      : ["library.bib"]

floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf
---

```{r global_options, include=FALSE}
# knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, 
#                       fig.pos = "tb", fig.path='figs/',
#                       echo=F, warning=F, cache=F, 
#                       message=F, sanitize = T)

# knitr::opts_chunk$set(echo = TRUE)
# knitr::opts_knit$set(root.dir = "")
getwd()
```

```{r libraries, echo=FALSE}
library(tidyverse)

# Regression and regression visualization tools
library(broom)
library(broom.mixed)
library(brms)
library(tidybayes)
library(lme4)
library(lmerTest)

# Plotting
library(ggbeeswarm)
```
```{r custom functions, include=FALSE}
prepare_demo = function(demo_df) {
  age_responses = demo_df %>% 
    filter(internal_node_id == "0.0-0.0") %>% 
    mutate(survey_answer=as.integer(survey_answer))
  native_responses = demo_df %>% 
    filter(trial_type == "survey-multi-choice", survey_question_idx == 1)
  
  return(list(
    N=nrow(age_responses),
    
    age_mean=mean(age_responses$survey_answer),
    age_sd=sd(age_responses$survey_answer),
    age_min=min(age_responses$survey_answer),
    age_max=max(age_responses$survey_answer),
    
    date_min=min(age_responses$dateTime),
    date_max=max(age_responses$dateTime)
  ))
}

demo_chunk = function(demo_df) {
  p = papaja::printnum
  demo = prepare_demo(demo_df)
  cat(paste0("We recruited ", demo$N, " self-reported native English speakers ",
             "(age range: ", p(demo$age_min), "--", p(demo$age_max),
             ", mean: ", p(demo$age_mean), ", SD: ", p(demo$age_sd),
             ") from Amazon Mechanical Turk. "))
  cat("Workers were admitted only if the acceptance rate of their past HITs exceeded 95%. ")
  
  min_date_str = strftime(demo$date_min, "%B %Y")
  max_date_str = strftime(demo$date_max, "%B %Y")
  cat("The data were collected ")
  if (min_date_str == max_date_str) {
    cat(paste0("in ", min_date_str, "."))
  } else {
    cat(paste0("between ", min_date_str, " and ", max_date_str, "."))
  }
}

# Pipe shared across experiments: merge in relevant experimental materials and
# filter out items based on constraints shared across experiments.
merge_materials_and_filter = function(df) {
  df %>% 
    left_join(materials_df %>% select(materials_id, id, 
                                      `A concrete?`, `A countable?`, `A animate?`,
                                      `L plural?`, A, V, L),
              by=c("materials_id", "item_id"="id")) %>% 
    
    # Retain only concrete experimental items.
    filter(condition_0 == "filler" | `A concrete?` == TRUE)
  
  # TODO drop observations on items that were only present in earlier materials
  # revisions.
}

# Run a one-tailed t-test on filler responses within uniqueid, testing whether
# responses for some "high" condition exceed responses for some "low" condition.
# Adds at least two columns to the provided df:
#   - `exclude`: logical, TRUE iff uniqueid t-test passed
#   - `p.value`: float, p-value of t-test
run_slider_exclusions = function(slider_filler_df, condition_variable, 
                                 high_condition, low_condition, alpha=0.1) {
  ret = slider_filler_df %>% 
    group_by(uniqueid, {{condition_variable}}) %>% 
      nest() %>% 
        spread(key={{condition_variable}}, value=data) %>% 
        mutate(t_test=map2({{high_condition}}, {{low_condition}}, ~{
          tryCatch(
            {t.test(.x$slider_value, .y$slider_value, alternative="greater") %>% tidy()},
            error=function(e) {
              if (grepl("constant", as.character(e))) {
                # Responses are near-constant within-group. That could mean that
                # the subject is really consistent within-group and there is a
                # between-group difference, or they are simply responding the
                # same across trials. In the former case, we return a successful
                # p-value; in the latter, unsuccessful.
                if (mean(.x$slider_value) > mean(.y$slider_value)) {
                  tibble(p.value=0)
                } else {
                  tibble(p.value=1)
                }
              } else if (grepl("not enough", as.character(e))) {
                # Not enough observations in one of the groups. Subject may have
                # copped out a lot. Verify this; if true, return p=1 to exclude.
                # Otherwise reraise.
                if (length(na.omit(.x$slider_value)) <= 1 ||
                    length(na.omit(.y$slider_value)) <= 1) {
                  warning("Subject had <= 1 non-copout filler responses. Excluding.")
                  tibble(p.value=1)
                } else {
                  # reraise
                  stop(e)
                }
              } else {
                # reraise
                stop(e)
              }
            }
          )
        })) %>% 
        select(-{{high_condition}}, -{{low_condition}}) %>% 
      unnest(t_test) %>% 
    mutate(exclude=p.value >= alpha)
  
  return(ret)
}

# Conduct a mixed-effects linear regression on Likert-scale acceptability
# responses in variable `slider_value`, contrasting expected "good" and "bad"
# condition.
# This contrast is treatment-coded, with the "bad" condition as the reference
# level.
# 
# Returns the model along with a dataframe describing "controlled"
# data points subtracting estimated random effects. This is useful for
# visualization.
#
# Returns a list containing
#   - `exp_conditions`: character vector
#   - `bad_condition`: character
#   - `model`: regression model
#   - `df_controlled`: `df` filtered to contain the relevant conditions, and
#     containing a new column `slider_value_controlled` which contains
#     `slider_value - ranef`, where `ranef` for a given row is the sum of
#     subject and item random effects estimated in the regression.
do_acceptability_regression = function(df, condition_col, exp_conditions, bad_condition) {
  df = df %>% filter({{condition_col}} %in% c(bad_condition, exp_conditions)) %>% 
    mutate(contrast = relevel(as.factor({{condition_col}}), ref=bad_condition))
  
  # DEV: leaving out control variables because they're not defined for fillers.
  # model = lmer(slider_value ~ contrast * `A animate?` 
  #                           + contrast * `A countable?`
  #                           + (1|item_id)
  #                           + (contrast * `A animate?` + contrast * `A countable?`|uniqueid),
  #              data=df,
  #              contrasts=list(contrast=c(`FALSE`=-1, `TRUE`=1),
  #                             `A animate?`=c(-1, 1),
  #                             `A countable?`=c(-1, 1)),
  #              control=lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=100000)))
    model = lmer(slider_value ~ contrast
                            + (contrast|item_id)
                            + (contrast|uniqueid),
               data=df,
               contrasts=list(contrast=contr.treatment(levels(df$contrast)),
                              `A animate?`=c(-1, 1),
                              `A countable?`=c(-1, 1)),
               control=lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=100000)))
  
  # Compute item responses after controlling for random effects.
  slider_value_random = predict(model, random.only=T)
  df_controlled = cbind(df, slider_value_random) %>%
    mutate(slider_value_controlled = slider_value - slider_value_random)

  return(list(exp_conditions=exp_conditions, bad_condition=bad_condition,
              model=model, df_controlled=df_controlled))
}
```
```{r shared chunks, include=FALSE}
describe_stimuli_two_sentence = function() {
  return("We presented subjects with the two-sentence stimuli described in \\@ref(stimuli). We manipulated the two relevant factors (*Given entity*, determined by the first sentence, establishing either the Agent/Theme or Location as a referent; and *Construction type*, evident in the second sentence, using either the A-construction or L-construction). Both manipulations were within-subject and within-item, pseudo-randomly counterbalanced between subjects.")
}
```
```{r load raw data, echo=FALSE}
DATA_VERSION = "2021-11-11-1440"
raw_df <- read_csv(paste0("../../data/raw_data.", DATA_VERSION, ".csv"),
                   na=c("", "nan"),
                   col_types=cols()) %>% 
  mutate(dateTime=lubridate::as_datetime(dateTime / 1000),
         item_id=as.factor(as.integer(item_id)))
```
```{r load materials, echo=FALSE}
all_items = raw_df %>% select(materials_id, item_id) %>% distinct()
load_json = function(materials_id) {
  jsonlite::read_json(paste0("../../materials/", materials_id, ".json"),
                         simplifyVector=T)$items %>% 
        add_column(materials_id=materials_id, .before=0) %>% 
        mutate(id=as.factor(as.integer(id))) %>% 
        relocate(id, .after=materials_id) %>% 
        type_convert()
}

materials_df <- na.omit(unique(all_items$materials_id)) %>% 
  map_dfr(possibly(load_json, otherwise=data.frame(), quiet=FALSE))
```


## Procedure

| $\rightarrow$ Given entity <br/> $\downarrow$ Construction | Agent/Theme | Location |
| - | - | - |
| A-construction | Bees! They are swarming in the garden. | The garden! Bees are swarming in it. |
| L-construction | Bees! The garden is swarming with them. | The garden! It is swarming with bees. | 

Table: An example experimental item varying across two factors: Given entity (Agent/Theme vs. Location) and Construction (A-construction vs. L-construction). TODO Bring in real prefix sentence, and maybe pick a better one. (ref:item-example)

(ref:stimuli) Participants were shown English passages consisting of two sentences. Each passage varied within-participant and within-item by two factors: *Given entity* and *Construction type*. An example item in four varying conditions is shown in \@ref(item-example).

The first sentence of each item established one of two possible event participants as a discourse referent: either the agent/theme (*Bees*) or the location (*The garden*), usually expressed using the exact same lexical item as in the following sentence, and in varying syntactic positions. (TODO introduce notion of establishing discourse referent.)

The second sentence then used either the A-construction or L-construction to express the relevant relation between Agent/Theme and Location. The participant established as given in the first sentence was expressed pronominally in the second sentence. Non-pronominal Agent/Theme participants were realized as bare NPs, while non-pronominal locations were realized with definite determiners (*the* or *his/her/their* depending on the context). The A-construction used locative prepositions matching the path and manner of motion coded by the verb. 

### Experimental structure

| Experiment | Behavior | Guiding question |
| - | - | - |
| [1](#experiment-swarm-comprehension-full) | Comprehension | Are comprehenders' meaning inferences sensitive to evident features of discourse structure? |
| C1 | Acceptability | Are A-constructions and L-constructions accepted by speakers? |
| C2 | Comprehension | Do comprehenders infer a meaning contrast between A-constructions and L-constructions? |
| [C3](#experiment-swarm-comprehension-control) | Comprehension | Do comprehenders infer a meaning contrast from our manipulations intended to influence discourse structure? |
| C4 | Production | Do speakers select between A-constructions and L-constructions based on discourse structure? |

Table: Overview of experiments.

## Experiments

### Acceptability experiment {#experiment-swarm-acceptability}

We first confirmed that our uses of the A-construction and L-construction were deemed acceptable by English speakers.

```{r load 02 data}
experiment_name = "02_acceptability_swarm"

df.02 = raw_df %>%
  filter(experiment_id == experiment_name,
         trial_type == "html-slider-response-with-copout") %>% 
  mutate(agent_is_subject = ifelse(condition_1 %in% c(0, 1), condition_1 == 1, NA)) %>% 
  
  merge_materials_and_filter

# Demographic information
df.02.demo = raw_df %>% 
  filter(experiment_id == experiment_name,
         trial_type %in% c("survey-text", "survey-multi-choice"))
```

#### Participants

```{r, results='asis'}
# TODO exclude native speakers
demo_chunk(df.02.demo)
```

#### Procedure

Subjects were presented with sentences drawn from our materials using either the A-construction or the L-construction, and asked to rate how "natural" each sentence was in each trial on a 7-point Likert scale. Each subject completed several practice trials prior to the experiment designed to calibrate this notion of a "natural" sentence.
We varied construction type both within-subject and within-item, and the assignment was pseudo-randomly counterbalanced across subjects.
Subjects were also presented with grammatical and ungrammatical filler sentences. Filler and experimental trials were pseudo-randomly shuffled

#### Exclusion criteria

```{r 02 exclusion}
subject.ttest.02 = run_slider_exclusions(
  df.02 %>% filter(condition_0 == "filler"), condition_1, good, bad)
Nretained.02 = nrow(subject.ttest.02 %>% filter(!exclude))

df.02.filtered = df.02 %>%
  left_join(subject.ttest.02 %>% select(uniqueid, exclude), by=c("uniqueid")) %>% 
  filter(exclude == FALSE)
```

We used subjects' performance on filler items to design an exclusion criterion. For each subject, we collect their Likert responses on filler items, which have ground-truth "acceptable" and "unacceptable" labels. We then perform a two-sample one-tailed $t$-test between these groups. We retain only those subjects for which we find that $\text{rating}_\text{acceptable} > \text{rating}_\text{unacceptable}$ with one-tailed $p < 0.1$.

`r Nretained.02` subjects (`r Nretained.02 / nrow(subject.ttest.02) * 100`%) were retained by this criterion.

#### Results

```{r 02 slider visualization, cache=TRUE}
df.02 %>%
  
  # Join in filler t-test results
  full_join(select(subject.ttest.02, uniqueid, exclude, p.value) %>% rename(filler.p.value=p.value),
            by=c("uniqueid")) %>% 
  
  mutate(
    # Reorder subjects based on filler test p-value
    uniqueid=fct_reorder(uniqueid, filler.p.value)) %>%
  
  ggplot(aes(x=uniqueid, y=slider_value, color=condition_1)) +
    
    geom_quasirandom(groupOnX=TRUE, alpha=0.4, data=. %>% filter(condition_1 %in% c(0, 1))) +
    scale_color_discrete(name="Stimulus condition",
                         labels=c(`0`="L-construction", `1`="A-construction",
                                  "bad"="Ungrammatical filler", "good"="Grammatical filler")) +
  
    # Plot boxplots for fillers
    geom_boxplot(alpha=0.2, data=. %>% filter(condition_1 == "good")) +
    geom_boxplot(alpha=0.2, data=. %>% filter(condition_1 == "bad")) +
  
    # Shade based on exclusion
    geom_rect(data=. %>% group_by(uniqueid) %>% filter(row_number() == 1),
              aes(xmin = as.numeric(uniqueid) - 0.5, xmax = as.numeric(uniqueid) + 0.5,
                  ymin = 1, ymax = 7, fill = exclude),
              alpha = 0.1, color=NA) +
    scale_fill_discrete(name="Excluded?", labels=c(`FALSE`="No", `TRUE`="Yes")) +
  
    scale_x_discrete(name="Subject", labels=1:nrow(subject.ttest.02)) +
    scale_y_continuous(name="Acceptability rating", breaks=c(1, 2, 3, 4, 5, 6, 7))
```

We evaluated the effect of our experimental manipulation using a mixed-effects linear regression model, comparing acceptability ratings on each construction (A-construction or L-construction) to ungrammatical items. The fixed effect of interest was this *Contrast* factor (treatment-coded; ungrammatical items = 0, A-construction = 1, L-construction = 1). We included as sum-coded covariates *Countability* (whether the agent/theme was a mass or count noun) and *Animacy* (whether the agent/theme expressed an animate participant). Finally, we included interaction terms between these control variables and the *Contrast* factor.
We used a maximal random effect structure, including all logically possible by-subject and by-item random slopes and intercepts.

```{r 02 regression, echo=FALSE}
# TODO acceptability regression incorporates animacy and countability, but 1) these aren't defined for fillers and 2) filler materials ID didn't get recorded correctly for early experiments.
# #1 is easy to fix. #2 we need to recover the correct materials for fillers
# For now, going to leave out those control variables.

model.02 = xfun::cache_rds({do_acceptability_regression(df.02.filtered, condition_1, c(0, 1), bad_condition)})
model.02.print = papaja::apa_print(model.02$model)
```

```{r 02 regression results, results="asis"}
papaja::apa_table(model.02.print$table, caption="Acceptability regression results")
```

The regression shows a significant main effect relative to ungrammatical English items for both constructions (A-construction: `r model.02.print$statistic$contrast1`; L-construction: `r model.02.print$statistic$contrast0`).

```{r, fig.cap="Acceptability responses for L-construction, A-construction, and ungrammatical filler items, after controlling for by-item and by-subject random effects."}
# TODO this is a very unintuitive graph, because it reveals how our regression assumptions are violated :)
# After controlling for estimated subject/item effets, some responses are higher than 7!
model.02$df_controlled %>% 
  ggplot(aes(x=condition_1, y=slider_value_controlled)) +
  geom_boxplot() +
  geom_quasirandom(alpha=0.3) +
  scale_y_continuous(name="Acceptability rating", breaks=c(1, 2, 3, 4, 5, 6, 7)) +
  scale_x_discrete(name="Condition", labels=c(`0`="L-construction", `1`="A-construction", "bad"="Ungrammatical fillers"))
```


#### Conclusion

TODO

### Two-sentence acceptability experiment {#experiment-swarm-acceptability-two}

Our full experiments use stimuli containing two sentences each. The first sentence establishes an entity as topical, and the second sentence uses the A-construction or L-construction with that entity expressed anaphorically. We next tested whether these two-sentence stimuli were natural for English speakers.

```{r load 08 data}
experiment_name = "08_acceptability_swarm-withprefix"

df.08 = raw_df %>%
  filter(experiment_id == experiment_name,
         trial_type == "html-slider-response-with-copout") %>% 
  mutate(agent_is_subject = ifelse(condition_1 %in% c(0, 1), condition_1 == 1, NA),
         agent_is_given = ifelse(condition_0 %in% c(0, 1), condition_0 == 0, NA),
         condition = ifelse(condition_0 == "filler", paste(condition_0, condition_1),
                            paste0("c", ifelse(agent_is_subject, "A", "L"),
                                   "g", ifelse(agent_is_given, "A", "L")))) %>% 
  
  merge_materials_and_filter

# Demographic information
df.08.demo = raw_df %>% 
  filter(experiment_id == experiment_name,
         trial_type %in% c("survey-text", "survey-multi-choice"))
```

#### Participants

```{r, results='asis'}
# TODO exclude native speakers
demo_chunk(df.08.demo)
```

#### Procedure

`r describe_stimuli_two_sentence()` Subjects were asked to rate how "natural" each passage was in each trial on a 7-point Likert scale. Each subject completed several practice trials prior to the experiment designed to calibrate this notion of a "natural" passage.
Subjects were also presented with grammatical and ungrammatical filler sentences. Filler and experimental trials were pseudo-randomly shuffled.

#### Exclusion criteria

```{r 08 exclusion}
subject.ttest.08 = run_slider_exclusions(
  df.08 %>% filter(condition_0 == "filler"), condition_1, good, bad)
Nretained.08 = nrow(subject.ttest.08 %>% filter(!exclude))

df.08.filtered = df.08 %>%
  left_join(subject.ttest.08 %>% select(uniqueid, exclude), by=c("uniqueid")) %>% 
  filter(exclude == FALSE)
```

We used subjects' performance on filler items to design an exclusion criterion. For each subject, we collect their Likert responses on filler items, which have ground-truth "acceptable" and "unacceptable" labels. We then perform a two-sample one-tailed $t$-test between these groups. We retain only those subjects for which we find that $\text{rating}_\text{acceptable} > \text{rating}_\text{unacceptable}$ with one-tailed $p < 0.1$.
`r Nretained.08` subjects (`r Nretained.08 / nrow(subject.ttest.08) * 100`%) were retained by this criterion.

#### Results

```{r 08 slider visualization, cache=TRUE}
df.08 %>%

  # Join in filler t-test results
  full_join(select(subject.ttest.08, uniqueid, exclude, p.value) %>% rename(filler.p.value=p.value),
            by=c("uniqueid")) %>% 
  
  mutate(
    # Reorder subjects based on filler test p-value
    uniqueid=fct_reorder(uniqueid, filler.p.value),
    
    plot_group=ifelse(condition_0 == "filler", condition_1,
                      "exp")) %>%
  
  ggplot(aes(x=uniqueid, y=slider_value, color=plot_group)) +
    
    geom_quasirandom(groupOnX=TRUE, alpha=0.4, data=. %>% filter(plot_group == "exp")) +
    scale_color_discrete(name="Stimulus type",
                         labels=c("exp"="Experimental item",
                                  "bad"="Ungrammatical filler", "good"="Grammatical filler")) +
  
    # Plot boxplots for fillers
    geom_boxplot(alpha=0.2, data=. %>% filter(condition_1 == "good")) +
    geom_boxplot(alpha=0.2, data=. %>% filter(condition_1 == "bad")) +
  
    # Shade based on exclusion
    geom_rect(data=. %>% group_by(uniqueid) %>% filter(row_number() == 1),
              aes(xmin = as.numeric(uniqueid) - 0.5, xmax = as.numeric(uniqueid) + 0.5,
                  ymin = 1, ymax = 7, fill = exclude),
              alpha = 0.1, color=NA) +
    scale_fill_discrete(name="Excluded?", labels=c(`FALSE`="No", `TRUE`="Yes")) +
  
    scale_x_discrete(name="Subject", labels=1:nrow(subject.ttest.08)) +
    scale_y_continuous(name="Acceptability rating", breaks=c(1, 2, 3, 4, 5, 6, 7))
```

```{r 08 regression, echo=FALSE}
model.08 = xfun::cache_rds({
  do_acceptability_regression(df.08.filtered, condition, c("cAgA", "cAgL", "cLgL", "cLgA"), "filler bad")
})

model.08.print = papaja::apa_print(model.08$model)
```

```{r 08 boxplots, fig.cap="Acceptability responses for two-sentence stimuli and ungrammatical filler items, after controlling for by-item and by-subject random effects."}
# TODO this is a very unintuitive graph, because it reveals how our regression assumptions are violated :)
# After controlling for estimated subject/item effects, some responses are higher than 7!
model.08$df_controlled %>% 
  ggplot(aes(x=condition, y=slider_value_controlled)) +
  geom_boxplot() +
  geom_quasirandom(alpha=0.3) +
  scale_y_continuous(name="Acceptability rating", breaks=c(1, 2, 3, 4, 5, 6, 7))

  # TODO make this scale
  # scale_x_discrete(name="Condition", labels=c(`0`="L-construction", `1`="A-construction", "bad"="Ungrammatical fillers"))
```

### Basic comprehension experiment {#experiment-swarm-comprehension-basic}

To the best of our knowledge, no empirical test of the meaning contrast between the A- and L-construction exists. We first ran an experiment to test whether English speakers recognized this meaning contrast.

```{r load 00 data}
experiment_name = "00_comprehension_swarm-construction-meaning"
num_fillers.00 = 12

df.00 = raw_df %>%
  filter(experiment_id == experiment_name,
         trial_type == "html-slider-response-with-copout") %>% 
  mutate(location_is_subject = ifelse(condition_1 %in% c(0, 1), condition_1 == 0, NA)) %>%
  
  merge_materials_and_filter

# Drop subjects from early pilot trials which did not include the full set of
# fillers.
df.00.drops = df.00 %>% 
  filter(condition_0 == "filler") %>% 
  group_by(uniqueid) %>% 
    summarise(n=n()) %>% 
  filter(n != num_fillers.00)
df.00 = df.00 %>% anti_join(df.00.drops, by=c("uniqueid"))

# Demographic information
df.00.demo = raw_df %>% 
  # Perform same exclusion as above
  anti_join(df.00.drops, by=c("uniqueid")) %>% 
  filter(experiment_id == experiment_name,
         trial_type %in% c("survey-text", "survey-multi-choice"))
```

#### Participants

```{r, results='asis'}
# TODO exclude native speakers
demo_chunk(df.00.demo)
```

#### Procedure

Subjects were presented with sentences drawn from our materials using either the A-construction or L-construction. This construction type was varied both within-subject and within-item, and the assignment was pseudo-randomly counterbalanced across subjects.

TODO describe fillers.

In each trial, subjects were asked: "How much/many \<Agent/Theme> is/are in/on \<Location>?" They responded using a slider interface whose value ranged from 0 to 100. The ends of the slider were labeled to suggest a scale relative to the size of the Location, where a value of 100 indicates that the Location is as full as it could be of the Agent/Theme, and a value of 0 indicates that the Location contains none of the Agent/Theme.

TODO include vignette.

TODO copout.

#### Exclusion criteria {#swarm-slider-exclusion-criteria}

The filler items of the experiment all denoted scenes with clearly empty or full locations. We used subjects' responses on these filler items to check their attention and design an exclusion criterion. For each subject, we collect their responses on filler items with ground-truth "full" locations and ground-truth "empty" locations. We then perform a two-sample one-tailed $t$-test between the two groups. We retain only those subjects for whom this test yields $\text{fullness}_\text{full} > \text{fullness}_\text{empty}$ with a one-tailed $p < 0.1$.

```{r 00 exclusion}
subject.ttest.00 = run_slider_exclusions(
  df.00 %>% filter(condition_0 == "filler"), condition_1, full, empty)
Nretained.00 = nrow(subject.ttest.00 %>% filter(!exclude))

df.00.filtered = df.00 %>% 
  left_join(subject.ttest.00 %>% select(uniqueid, exclude), by=c("uniqueid")) %>% 
  filter(exclude == FALSE)
```

`r Nretained.00` subjects (`r Nretained.00 / nrow(subject.ttest.00) * 100`%) were retained by this criterion.

#### Results

```{r 00 slider visualization, fig.cap="Visualization of slider response behavior by subject. Boxplots show response behavior on filler items; we expect filler responses to lie at the extremes of the scale. Swarmplots show responses on experimental items.", cache=TRUE}
df.00 %>%
  
  # Join in filler t-test results
  left_join(select(subject.ttest.00, uniqueid, exclude, p.value) %>% rename(filler.p.value=p.value),
            by=c("uniqueid")) %>% 
  
  mutate(
    # Reorder subjects based on p-value of filler t-test
    uniqueid=fct_reorder(uniqueid, filler.p.value),
    
    plot_group=ifelse(condition_0 == "filler", condition_1,
                      "exp")) %>%
  
  ggplot(aes(x=uniqueid, y=slider_value, color=plot_group)) +
    
    geom_quasirandom(groupOnX=TRUE, alpha=0.4, data=. %>% filter(plot_group == "exp")) +
    scale_color_discrete(name="Stimulus type",
                         labels=c("exp"="Experimental item",
                                  "empty"="Empty filler", "full"="Full filler")) +
  
    # Plot boxplots for fillers
    geom_boxplot(alpha=0.2, data=. %>% filter(condition_1 == "full")) +
    geom_boxplot(alpha=0.2, data=. %>% filter(condition_1 == "empty")) +
  
    # Shade based on exclusion
    geom_rect(data=. %>% group_by(uniqueid) %>% filter(row_number() == 1),
              aes(xmin = as.numeric(uniqueid) - 0.5, xmax = as.numeric(uniqueid) + 0.5,
                  ymin = 0, ymax = 100, fill = exclude),
              alpha = 0.1, color=NA) +
    scale_fill_discrete(name="Excluded?", labels=c(`FALSE`="No", `TRUE`="Yes")) +
  
    scale_x_discrete(name="Subject", labels=1:nrow(subject.ttest.00)) +
    scale_y_continuous(name="Fullness rating") +
    theme()
```


We evaluated the effect of our experimental manipulation using a Bayesian mixed-effects beta regression. We entered the manipulation *Construction type* (sum-coded; A-construction = -1, L-construction = 1) as a fixed effect in the regression.
We used a maximal random effect structure, including all logically possible by-subject and by-item random slopes and intercepts.

```{r 00 regression, echo=FALSE}
contrasts(df.00.filtered$location_is_subject) = c(-1, 1)

model.00 = xfun::cache_rds({
  brm(slider_value ~ location_is_subject
                   + (location_is_subject | item_id)
                   + (location_is_subject | uniqueid),
      data=df.00.filtered %>%
        filter(condition_0 != "filler") %>% 
        mutate(slider_value = (slider_value + 1e-4) / (100 + 2 * 1e-4)),
      family="beta")
})
```

```{r 00 controlled figure}
# TODO need to implement the same logic for brms fits, which don't support predict() with random_only
# Not confident in the below code. the range of yielded values doesn't make sense
model.00 %>% tidybayes::add_predicted_draws(newdata=model.frame(model.00) %>% mutate(location_is_subject=NA)) %>% 
  group_by(uniqueid, item_id) %>% 
  summarise(slider_value_controlled=mean(slider_value) - mean(.prediction)) %>% 
  left_join(model.frame(model.00) %>% select(uniqueid, item_id, location_is_subject), by=c("uniqueid", "item_id")) %>% 
  
  ggplot(aes(x=location_is_subject, y=slider_value_controlled)) +
    geom_quasirandom(alpha=0.4) +
    geom_boxplot(alpha=0.5)
```


TODO report results

#### Conclusion

### Production experiment {#experiment-swarm-production-givenness}

```{r load 03 data}
experiment_names = c("03_production_swarm-givenness")

df.03 = raw_df %>%
  filter(experiment_id %in% experiment_names,
         trial_type == "survey-multi-choice-ext") %>% 
  
  # TODO double check agent_is_given coding here.
  mutate(agent_is_given = ifelse(condition_0 %in% c(0, 1), condition_0 == 1, NA),
         agent_is_subject = ifelse(condition_0 == "filler", NA, survey_answer == "agent")) %>% 
  
  merge_materials_and_filter

# Demographic information
df.03.demo = raw_df %>% 
  # Perform same exclusion as above
  # anti_join(df.04.drops, by=c("uniqueid")) %>%
  filter(experiment_id %in% experiment_names,
         trial_type %in% c("survey-text", "survey-multi-choice"))
```


We next confirmed that speakers were willing to select between the A-construction and L-construction based on given entities in the discourse.
TODO ref theoretical reason to expect this.
This has not been empirically confirmed for these particular constructions. TODO what are the reasons to think *a priori* that realizing as subject might be under different pressures than alternation of postverbal content as in the dative alternation?

#### Participants

```{r, results='asis'}
# TODO exclude native speakers
demo_chunk(df.03.demo)
```

#### Procedure

This experiment used the complete two-sentence stimuli described in \@ref(stimuli). Subjects performed forced-choice judgments over pairs of these stimuli which differed only in their second sentence. Thus each trial held a *Given entity* fixed while contrasting the two construction types. The factor of *Given entity* was manipulated within-subject and within-item, and assignment was pseudo-randomly counterbalanced among subjects.

We asked subjects to judge which was the more "natural" of the two stimuli. The experiment contained TODO filler items which contrasted grammatical and ungrammatical (or highly unnatural) English text.

#### Exclusion criteria {#swarm-2afc-exclusion-criteria}

TODO what did we do here?

```{r}
# Exclude subjects whose success rate is lower than this threshold
exclude_threshold.03 = 0.7

# Early phases of data collection revealed that some fillers don't illustrate
# the contrast appropriately. (Subjects otherwise performing well missed these
# items.) We'll drop them from the exclusion routine.
drop_fillers.03 = c(6)

subject.exclude.03 = df.03 %>%
  filter(
    condition_0 == "filler",
    !(item_id %in% drop_fillers.03)) %>% 
  mutate(success=survey_answer == "good") %>% 
  group_by(uniqueid) %>% 
    summarise(success_rate=mean(success)) %>% 
  mutate(exclude=success_rate < exclude_threshold.03)
Nretained.03 = nrow(subject.exclude.03 %>% filter(!exclude))

df.03.filtered = df.03 %>% 
  left_join(subject.exclude.03, by=c("uniqueid")) %>% 
  filter(!exclude)
```

`r Nretained.03` subjects (`r Nretained.03 / nrow(subject.exclude.03) * 100`%) were retained by this criterion.

#### Results

```{r}
# TODO render controlled response figure
```

```{r 03 regression, echo=FALSE}
lmer.03 = xfun::cache_rds({
  glmer(agent_is_subject ~ agent_is_given
                         + (agent_is_given | item_id) 
                         + (agent_is_given | uniqueid),
        family="binomial",
        data=df.03.filtered %>% 
          filter(condition_0 != "filler"),
        control = glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun=2e5)),
        contrasts=list(agent_is_given=c(-1, 1)))
})
```


#### Conclusion

### Control comprehension experiment {#experiment-swarm-comprehension-control}

### Critical explaining-away experiment {#experiment-swarm-comprehension-full}

```{r load 04 data}
experiment_names = c("04_comprehension_swarm-full", "09_comprehension_swarm_full-nonalternating-control")
num_fillers.04 = 12

df.04 = raw_df %>%
  filter(experiment_id %in% experiment_names,
         trial_type == "html-slider-response-with-copout",
         # Drop nonalternating control trials.
         condition_1 != 2) %>% 
  
  # TODO double double check condition_1 codes here, including consistency between two experiment runs
  mutate(location_is_subject = ifelse(condition_1 %in% c(0, 1), condition_1 == 0, NA),
         agent_is_given = ifelse(condition_0 %in% c(0, 1), condition_0 == 0, NA),
         condition = ifelse(condition_0 == "filler", paste(condition_0, condition_1),
                            paste0("c", ifelse(location_is_subject, "L", "A"),
                                   "g", ifelse(agent_is_given, "A", "L")))) %>% 
  
  merge_materials_and_filter

# Drop subjects with the wrong number of filler items.
# TODO what is causing this problem in the first place?
df.04.drops = df.04 %>% 
  filter(condition_0 == "filler") %>% 
  group_by(uniqueid) %>% 
    summarise(n=n()) %>% 
  filter(n != num_fillers.04)
df.04 = df.04 %>% anti_join(df.04.drops, by=c("uniqueid"))

# Demographic information
df.04.demo = raw_df %>% 
  # Perform same exclusion as above
  anti_join(df.04.drops, by=c("uniqueid")) %>%
  filter(experiment_id %in% experiment_names,
         trial_type %in% c("survey-text", "survey-multi-choice"))
```


\@ref(experiment-swarm-comprehension-basic) confirmed that English speakers infer a meaning contrast between the A-construction and L-construction. \@ref(experiment-swarm-production) further confirmed that English speakers know to select between the A-construction and L-construction in response to features of the discourse. In this final experiment, we test whether English speakers use this knowledge about the multiple pressures faced by the speaker in selecting a construction to resolve potential meaning ambiguities.

#### Participants

```{r, results='asis'}
# TODO exclude native speakers
demo_chunk(df.04.demo)
```

#### Procedure

`r describe_stimuli_two_sentence()`
We used the same slider interface as described in \@ref(experiment-swarm-comprehension-basic). An example trial is shown in TODO.

TODO filler items

TODO copout

#### Exclusion criteria

We used the same exclusion criteria as in \@ref(experiment-swarm-comprehension-basic).

```{r 04 exclusion}
subject.ttest.04 = run_slider_exclusions(
  df.04 %>% filter(condition_0 == "filler"), condition_1, full, empty)
Nretained.04 = nrow(subject.ttest.04 %>% filter(!exclude))

df.04.filtered = df.04 %>% 
  left_join(subject.ttest.04 %>% select(uniqueid, exclude), by=c("uniqueid")) %>% 
  filter(exclude == FALSE)
```

`r Nretained.04` subjects (`r Nretained.04 / nrow(subject.ttest.04) * 100`%) were retained by this criterion.

#### Results

```{r 04 slider visualization, fig.cap="Visualization of slider response behavior by subject. Boxplots show response behavior on filler items; we expect filler responses to lie at the extremes of the scale. Swarmplots show responses on experimental items.", cache=TRUE}
df.04 %>%

  # Join in filler t-test results
  full_join(select(subject.ttest.04, uniqueid, exclude, p.value) %>% rename(filler.p.value=p.value),
            by=c("uniqueid")) %>% 
  
  mutate(
    # Reorder subjects based on filler test p-value
    uniqueid=fct_reorder(uniqueid, filler.p.value),
    
    plot_group=ifelse(condition_0 == "filler", condition_1,
                      "exp")) %>%
  
  ggplot(aes(x=uniqueid, y=slider_value, color=plot_group)) +
    
    geom_quasirandom(groupOnX=TRUE, alpha=0.4, data=. %>% filter(plot_group == "exp")) +
    scale_color_discrete(name="Stimulus type",
                         labels=c("exp"="Experimental item",
                                  "empty"="Empty filler", "full"="Full filler")) +
  
    # Plot boxplots for fillers
    geom_boxplot(alpha=0.2, data=. %>% filter(condition_1 == "full")) +
    geom_boxplot(alpha=0.2, data=. %>% filter(condition_1 == "empty")) +
  
    # Shade based on exclusion
    geom_rect(data=. %>% group_by(uniqueid) %>% filter(row_number() == 1),
              aes(xmin = as.numeric(uniqueid) - 0.5, xmax = as.numeric(uniqueid) + 0.5,
                  ymin = 0, ymax = 100, fill = exclude),
              alpha = 0.1, color=NA) +
    scale_fill_discrete(name="Excluded?", labels=c(`FALSE`="No", `TRUE`="Yes")) +
  
    scale_x_discrete(name="Subject", labels=1:nrow(subject.ttest.04)) +
    scale_y_continuous(name="Fullness rating")
```

We evaluated the effect of our experimental manipulation using a Bayesian mixed-effects beta regression. We entered the manipulations *Given entity* (sum-coded; Location = -1, Agent/Theme = 1) and *Construction type* (sum-coded; A-construction = -1, L-construction = 1) as fixed effects in the regression, along with the interaction of the two variables. (Note that both manipulations are coded such that we should expect a higher rating on the fullness measure for positive values of the factor.)
We used a maximal random effect structure, including all logically possible by-subject and by-item random slopes and intercepts.

```{r 04 regression, echo=FALSE}
contrasts(df.04.filtered$location_is_subject) = c(-1, 1)
contrasts(df.04.filtered$agent_is_given) = c(-1, 1)

model.04 = xfun::cache_rds({
  brm(slider_value ~ location_is_subject * agent_is_given
                   + (location_is_subject * agent_is_given | item_id)
                   + (location_is_subject * agent_is_given | uniqueid),
      data=df.04.filtered %>%
        filter(condition_0 != "filler") %>% 
        mutate(slider_value = (slider_value + 1e-4) / (100 + 2 * 1e-4)),
      family="beta")
})
```
```{r 04 HDIs}
model.04.fixef_draws = model.04 %>%
  spread_draws(b_location_is_subject1, b_agent_is_given1)
model.04.hdis = model.04.fixef_draws %>%
  select(-starts_with(".")) %>% map(~{hdi(.)})
model.04.hdis
```


# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
