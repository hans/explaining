---
title             : "The title"
shorttitle        : "Title"

author:
  - name          : "First Author"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Postal address"
    email         : "my@email.com"
    role:         # Contributorship roles (e.g., CRediT, https://casrai.org/credit/)
      - Conceptualization
      - Writing - Original Draft Preparation
      - Writing - Review & Editing
  - name          : "Ernst-August Doelle"
    affiliation   : "1,2"
    role:
      - Writing - Review & Editing

affiliation:
  - id            : "1"
    institution   : "Wilhelm-Wundt-University"
  - id            : "2"
    institution   : "Konstanz Business School"

authornote: |
  Add complete departmental affiliations for each author here. Each new line herein must be indented, like this line.

  Enter author note here.

abstract: |
  Abstract belongs here.

keywords          : "keywords"
wordcount         : "X"

bibliography      : ["library.bib"]

floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "doc"
output            :
  papaja::apa6_pdf:
    toc: true
    number_sections: true
---

```{r global_options, include=FALSE}
# knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F,
#                       fig.pos = "tb", fig.path='figs/',
#                       echo=F, warning=F, cache=F,
#                       message=F, sanitize = T)

# knitr::opts_chunk$set(echo = TRUE)
# knitr::opts_knit$set(root.dir = "")
options(xfun.cache_rds.clean = FALSE)
```

```{r libraries, echo=FALSE}
library(tidyverse)

# Regression and regression visualization tools
library(broom)
library(broom.mixed)
library(brms)
library(tidybayes)
library(lme4)
library(lmerTest)

# Plotting
library(ggbeeswarm)
library(ggtext)

# APA
library(papaja)
```
```{r custom functions, include=FALSE}
prepare_demo = function(demo_df) {
  age_responses = demo_df %>%
    filter(internal_node_id == "0.0-0.0") %>%
    mutate(survey_answer=as.integer(survey_answer)) %>%
    filter(survey_answer < 100)
  native_responses = demo_df %>%
    filter(trial_type == "survey-multi-choice", survey_question_idx == 1)

  return(list(
    N=nrow(age_responses),

    age_mean=mean(age_responses$survey_answer),
    age_sd=sd(age_responses$survey_answer),
    age_min=min(age_responses$survey_answer),
    age_max=max(age_responses$survey_answer),

    date_min=min(age_responses$dateTime),
    date_max=max(age_responses$dateTime)
  ))
}

demo_chunk = function(demo_df) {
  p = papaja::printnum
  demo = prepare_demo(demo_df)
  cat(paste0("We recruited ", demo$N, " self-reported native English speakers ",
             "(age range: ", p(demo$age_min), "--", p(demo$age_max),
             ", mean: ", p(demo$age_mean), ", SD: ", p(demo$age_sd),
             ") from Amazon Mechanical Turk. "))
  cat("Workers were admitted only if the acceptance rate of their past HITs exceeded 95%. ")

  min_date_str = strftime(demo$date_min, "%B %Y")
  max_date_str = strftime(demo$date_max, "%B %Y")
  cat("The data were collected ")
  if (min_date_str == max_date_str) {
    cat(paste0("in ", min_date_str, "."))
  } else {
    cat(paste0("between ", min_date_str, " and ", max_date_str, "."))
  }
}

# Pipe shared across experiments: merge in relevant experimental materials and
# filter out items based on constraints shared across experiments.
merge_materials_and_filter = function(df) {
  df %>%
    left_join(materials_df %>% select(materials_id, id,
                                      `A concrete?`, `A countable?`, `A animate?`,
                                      `L plural?`, A, V, L,
                                      `non alternating given A`),
              by=c("materials_id", "item_id"="id")) %>%

    # Retain only concrete experimental items.
    filter(condition_0 == "filler" | `A concrete?` == TRUE)

  # TODO drop observations on items that were only present in earlier materials
  # revisions.
}

# Run a one-tailed t-test on filler responses within uniqueid, testing whether
# responses for some "high" condition exceed responses for some "low" condition.
# Adds at least two columns to the provided df:
#   - `exclude`: logical, TRUE iff uniqueid t-test passed
#   - `p.value`: float, p-value of t-test
run_slider_exclusions = function(slider_filler_df, condition_variable,
                                 high_condition, low_condition, alpha=0.1) {
  ret = slider_filler_df %>%
    group_by(uniqueid, {{condition_variable}}) %>%
      nest() %>%
        spread(key={{condition_variable}}, value=data) %>%
        mutate(t_test=map2({{high_condition}}, {{low_condition}}, ~{
          tryCatch(
            {t.test(.x$slider_value, .y$slider_value, alternative="greater") %>% tidy()},
            error=function(e) {
              if (grepl("constant", as.character(e))) {
                # Responses are near-constant within-group. That could mean that
                # the subject is really consistent within-group and there is a
                # between-group difference, or they are simply responding the
                # same across trials. In the former case, we return a successful
                # p-value; in the latter, unsuccessful.
                if (mean(.x$slider_value) > mean(.y$slider_value)) {
                  tibble(p.value=0)
                } else {
                  tibble(p.value=1)
                }
              } else if (grepl("not enough", as.character(e))) {
                # Not enough observations in one of the groups. Subject may have
                # copped out a lot. Verify this; if true, return p=1 to exclude.
                # Otherwise reraise.
                if (length(na.omit(.x$slider_value)) <= 1 ||
                    length(na.omit(.y$slider_value)) <= 1) {
                  warning("Subject had <= 1 non-copout filler responses. Excluding.")
                  tibble(p.value=1)
                } else {
                  # reraise
                  stop(e)
                }
              } else {
                # reraise
                stop(e)
              }
            }
          )
        })) %>%
        select(-{{high_condition}}, -{{low_condition}}) %>%
      unnest(t_test) %>%
    mutate(exclude=p.value >= alpha)

  return(ret)
}

# Conduct a mixed-effects linear regression on Likert-scale acceptability
# responses in variable `slider_value`, contrasting expected "good" and "bad"
# condition.
# This contrast is treatment-coded, with the "bad" condition as the reference
# level.
#
# Returns the model along with a dataframe describing "controlled"
# data points subtracting estimated random effects. This is useful for
# visualization.
#
# Returns a list containing
#   - `exp_conditions`: character vector
#   - `bad_condition`: character
#   - `model`: regression model
#   - `df_controlled`: `df` filtered to contain the relevant conditions, and
#     containing a new column `slider_value_controlled` which contains
#     `slider_value - ranef`, where `ranef` for a given row is the sum of
#     subject and item random effects estimated in the regression.
do_acceptability_regression = function(df, condition_col, exp_conditions, bad_condition) {
  df = df %>% filter({{condition_col}} %in% c(bad_condition, exp_conditions)) %>%
    mutate(contrast = relevel(as.factor({{condition_col}}), ref=bad_condition))

  # DEV: leaving out control variables because they're not defined for fillers.
  # model = lmer(slider_value ~ contrast * `A animate?`
  #                           + contrast * `A countable?`
  #                           + (1|item_id)
  #                           + (contrast * `A animate?` + contrast * `A countable?`|uniqueid),
  #              data=df,
  #              contrasts=list(contrast=c(`FALSE`=-1, `TRUE`=1),
  #                             `A animate?`=c(-1, 1),
  #                             `A countable?`=c(-1, 1)),
  #              control=lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=100000)))
    model = lmer(slider_value ~ contrast
                            + (contrast|item_id)
                            + (contrast|uniqueid),
               data=df,
               contrasts=list(contrast=contr.treatment(levels(df$contrast)),
                              `A animate?`=c(-1, 1),
                              `A countable?`=c(-1, 1)),
               control=lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=100000)))

  # Compute item responses after controlling for random effects.
  slider_value_random = predict(model, random.only=T)
  df_controlled = cbind(df, slider_value_random) %>%
    mutate(slider_value_controlled = slider_value - slider_value_random)

  return(list(exp_conditions=exp_conditions, bad_condition=bad_condition,
              model=model, df_controlled=df_controlled))
}

#' Perform the exact binomial test with custom output format
#'
#' This function is a wrapper for \code{binom.test()} with a custom output format. Specifically, the function returns a dataframe with columns for the mean of the input vector as well as min and max values calculated from \code{binom.test}. This function is particularly useful in conjunction with the \code{stat_summary()} function in \pkg{ggplot2} for plotting summary statistics of binary data.
#' @param numeric.vector A binary numeric vector.
#' @examples
#' test.vector <- rep(c(1, 0), 25)
#'
#' reskew_binom_test(test.vector)
#' @export
reskew_binom_test <- function(numeric.vector) {

  # Perform the exact binomial test
  binom.out <- binom.test(sum(numeric.vector), length(numeric.vector))

  # Package mean, confidence interval min and max into a dataframe
  df <- data.frame(sum(numeric.vector)/length(numeric.vector),
                   binom.out$conf.int[1], binom.out$conf.int[2])
  colnames(df) <- c("y", "ymin", "ymax")

  # Return the dataframe
  return(df)
}
```
```{r shared plotting and printing functions, include=FALSE}
plot_acceptability_by_item = function(df, condition_variable, condition_labels) {
  df %>%
    ggplot(aes(x=item_id, y=slider_value, fill={{condition_variable}})) +
      stat_summary(data=. %>% filter(condition_0 != "filler"), fun=mean,
                   geom="bar", position="dodge") +
      stat_summary(data=. %>% filter(condition_0 != "filler"), fun.data=mean_se,
                   geom="errorbar", position="dodge") +

      stat_summary(aes(x=1, fill=NULL, color={{condition_variable}}),
                   data=. %>% filter(condition_1 == "good"), fun.data=mean_se,
                   geom="errorbar", xmin=0, xmax=Inf, alpha=0.5) +
      stat_summary(aes(x=1, fill=NULL, color={{condition_variable}}),
                   data=. %>% filter(condition_1 == "bad"), fun.data=mean_se,
                   geom="errorbar", xmin=0, xmax=Inf, alpha=0.5) +

      scale_x_discrete(name="Item") +
      scale_y_continuous(name="Acceptability rating", breaks=c(1, 2, 3, 4, 5, 6, 7)) +
      scale_fill_discrete(name="Condition", labels=condition_labels) +
      scale_color_discrete(name="Filler items", labels=c("bad"="Ungrammatical fillers",
                                                         "good"="Grammatical fillers"))
}

# Print statistics on a coefficient saved in a BRMS model fit.
# Prints MAP estimate along with 95% CrI.
print_brms_coef = function(model, variable) {
  coef_data = model %>% tidy() %>% filter(term == variable)
  return(paste0("$\\hat\\beta =$ ",
                printnum(as.numeric(coef_data %>% pull(estimate))),
                ", 95% CrI: [",
                printnum(as.numeric(coef_data %>% pull(conf.low))),
                ", ",
                printnum(as.numeric(coef_data %>% pull(conf.high))),
                "]"))
}
```
```{r shared chunks, include=FALSE}
describe_stimuli_two_sentence = function() {
  return("We presented subjects with the two-sentence stimuli described in \\@ref(stimuli). We manipulated the two relevant factors (*Given entity*, determined by the first sentence, establishing either the Agent/Theme or Location as a referent; and *Construction type*, evident in the second sentence, using either the A-construction or L-construction). Both manipulations were within-subject and within-item, pseudo-randomly counterbalanced between subjects.")
}
describe_copout = function() {
  return("Beneath the slider widget was a checkbox labeled \"This sentence doesn't make sense to me.\" If a subject checked this box, the slider widget would be disabled and they would be allowed to proceed to the next trial.")
  # TODO talk about what happened?
}
```
```{r load raw data, echo=FALSE}
DATA_VERSION = "2021-11-13-1126"
raw_df <- read_csv(paste0("../../data/raw_data.", DATA_VERSION, ".csv"),
                   na=c("", "nan"),
                   col_types=cols()) %>%
  mutate(dateTime=lubridate::as_datetime(dateTime / 1000),
         item_id=as.factor(as.integer(item_id)))
```
```{r load materials, echo=FALSE}
all_items = raw_df %>% select(materials_id, item_id) %>% distinct()
load_json = function(materials_id) {
  jsonlite::read_json(paste0("../../materials/", materials_id, ".json"),
                         simplifyVector=T)$items %>%
        add_column(materials_id=materials_id, .before=0) %>%
        mutate(id=as.factor(as.integer(id))) %>%
        relocate(id, .after=materials_id) %>%
        type_convert()
}

materials_df <- na.omit(unique(all_items$materials_id)) %>%
  map_dfr(possibly(load_json, otherwise=data.frame(), quiet=FALSE))
```

# Background

We next identified a factor which would influence construction choice but which
was unrelated to the speaker's intended meaning.
Speakers prefer to choose constructions which place entities already given in
the discourse early in the linear order of a sentence [@bock1980syntactic,@prat-sala2000discourse].
We exploited this bias as a second pressure on construction choice: by
manipulating which of the two entities (Agent/Theme or Location) was
referentially available in the discourse, we expected speakers to have different
preferences over the two construction options.

We accomplished this by combining the critical sentence with a prefix sentence
which mentioned either the Agent/Theme or Location of the relevant item:

1. (*Agent/Theme as given*) The town is trying to clean up the **debris.**
2. (*Location as given*) The town is trying to clean up the **river.**

These prefix sentences expressed the relevant entity in a variety of syntactic
positions (subject, object, or the subject of a complement clause), usually
using the exact same lexical content as in the critical sentence.

TODO: Figure out how to combine with item presentation below.

# Procedure

## Stimulus design {#stimuli}

| $\rightarrow$ Given entity <br/> $\downarrow$ Construction | Agent/Theme | Location |
| - | - | - |
| A-construction | The town is trying to clean up the debris. It is flowing in the river. | The town is trying to clean up the river. Debris is flowing in it. |
| L-construction | The town is trying to clean up the debris. The river is flowing with it. | The town is trying to clean up the river. It is flowing with debris. |

Table: (\#tab:item-example) An example experimental item varying across two factors: *Given entity* in the first sentence (Agent/Theme vs. Location) and *Construction type* of the second sentence (A-construction vs. L-construction).

Each experimental item related three elements: a **Verb** and two plausible event participants, an **Agent/Theme** and a **Location**. We selected verbs which we believed could naturally appear in both the A-construction and L-construction (but later verified this with acceptability rating studies; see Section \@ref(experiment-swarm-acceptability)). For each item we designed two stimulus sentences: a first sentence which established one of the two event participants as a referent (the *Given entity*) in the discourse, expressed in varying syntactic positions; and a second sentence which used the Verb in either the A-construction or L-construction (the *Construction type*) with the two participants.

The participant established as given in the first sentence was expressed pronominally in the second sentence. Non-pronominal Agent/Theme participants were realized as bare NPs, while non-pronominal locations were realized with definite determiners (*the* or *his/her/their* depending on the context). The A-construction used locative prepositions matching the path and manner of motion coded by the verb.

Table \@ref(tab:item-example) shows an example item in the four possible combinations of these conditions. We varied these factors of *Given entity* and *Construction type* both within-participant and within-item; these assignments were pseudo-randomly counterbalanced in all experiments.

```{r set up running example, echo=FALSE}
example = list(
  agthm="debris",
  location="river",

  Agiv="The town is trying to clean up the debris.",
  Lgiv="The town is trying to clean up the river.",

  Acons="Debris is flowing in the river.",
  Lcons="The river is flowing with debris.",

  prompt="How much debris is in the river?"
)

example$lbl_Agiv = paste0("Agent<br/>(*", example$Agiv, "*)")
example$lbl_Lgiv = paste0("Location<br/>(*", example$Lgiv, "*)")

example$lbl_Lcons = paste0("L-construction<br/>(*", example$Lcons, "*)")
example$lbl_Acons = paste0("A-construction<br/>(*", example$Acons, "*)")
```

| Section | Behavior | Guiding question |
| - | - | - |
| \@ref(experiment-swarm-comprehension-basic) | Comprehension | Do comprehenders infer a meaning contrast between A-constructions and L-constructions? |
| \@ref(experiment-swarm-production-givenness) | Production | Do speakers select between A-constructions and L-constructions based on discourse structure? |
| \@ref(experiment-swarm-comprehension-full) | Comprehension | Are comprehenders' meaning inferences sensitive to evident features of discourse structure? |
| \@ref(experiment-swarm-acceptability), \@ref(experiment-swarm-acceptability-two) | Acceptability | Are A-constructions and L-constructions accepted by speakers? |
| \@ref(experiment-swarm-comprehension-control) | Comprehension | Do comprehenders infer a meaning contrast from our manipulations intended to influence discourse structure? |

Table: Overview of experiments.

# Experiments

## Basic comprehension experiment {#experiment-swarm-comprehension-basic}

To the best of our knowledge, no empirical test of the meaning contrast between the A- and L-construction exists. We first ran an experiment to test whether English speakers recognized this meaning contrast.

```{r load 00 data}
experiment_name = "00_comprehension_swarm-construction-meaning"
fillers_id.00 = "fillers/swarm_comprehension-000-base"
num_fillers.00 = 12

df.00 = raw_df %>%
  filter(experiment_id == experiment_name,
         trial_type == "html-slider-response-with-copout") %>%
  mutate(location_is_subject = ifelse(condition_1 %in% c(0, 1), condition_1 == 0, NA)) %>%

  merge_materials_and_filter

# Drop subjects from early pilot trials which did not include the full set of
# fillers.
df.00.drops = df.00 %>%
  filter(condition_0 == "filler") %>%
  group_by(uniqueid) %>%
    summarise(n=n()) %>%
  filter(n != num_fillers.00)
df.00 = df.00 %>% anti_join(df.00.drops, by=c("uniqueid"))

# Demographic information
df.00.demo = raw_df %>%
  # Perform same exclusion as above
  anti_join(df.00.drops, by=c("uniqueid")) %>%
  filter(experiment_id == experiment_name,
         trial_type %in% c("survey-text", "survey-multi-choice"))
```

### Procedure

Subjects were presented with sentences drawn from our materials using either the A-construction or L-construction. This construction type was varied both within-subject and within-item, and the assignment was pseudo-randomly counterbalanced across subjects. The full list of experimental materials is shown in \@ref(tab:tbl comp 00 materials).

In each trial, subjects were asked: "How much/many \<Agent/Theme> is/are in/on \<Location>?" They responded using a slider widget whose value ranged from 0 to 100. The ends of the slider were labeled to suggest a scale relative to the size of the Location, where a value of 100 indicates that the Location is as full as it could be of the Agent/Theme, and a value of 0 indicates that the Location contains none of the Agent/Theme.
`r describe_copout()`

```{r 00 vignette, fig.cap="Trial interface for the basic comprehension experiment."}
knitr::include_graphics("./figs/vignette_00.png")
```

These experimental items (`r nrow(df.00 %>% filter(condition_0 != "filler") %>% select(item_id) %>% distinct())` total) were combined with `r nrow(df.00 %>% filter(condition_0 == "filler") %>% select(item_id) %>% distinct())` filler items, shown in \@ref(tab:tbl comp filler materials). Each filler item presented a single sentence designed to denote a scene with an empty Location (e.g. *`r materials_df %>% filter(materials_id == fillers_id.00, id == 0) %>% pull(sentence)`*) or a full Location (e.g. *`r materials_df %>% filter(materials_id == fillers_id.00, id == 8) %>% pull(sentence)`*).

### Participants

```{r, results='asis'}
# TODO exclude native speakers
demo_chunk(df.00.demo)
```

### Exclusion criteria {#swarm-slider-exclusion-criteria}

We used subjects' responses on filler items to check their attention and design an exclusion criterion. For each subject, we collect their responses on filler items with ground-truth "full" Locations and ground-truth "empty" Locations. We then perform a two-sample one-tailed $t$-test between the two groups. We retain only those subjects for whom this test yields $\text{fullness}_\text{full} > \text{fullness}_\text{empty}$ with a one-tailed $p < 0.1$.

```{r 00 exclusion, echo=FALSE}
subject.ttest.00 = run_slider_exclusions(
  df.00 %>% filter(condition_0 == "filler"), condition_1, full, empty)
Nretained.00 = nrow(subject.ttest.00 %>% filter(!exclude))

df.00.filtered = df.00 %>%
  left_join(subject.ttest.00 %>% select(uniqueid, exclude), by=c("uniqueid")) %>%
  filter(exclude == FALSE)
```

`r Nretained.00` subjects (`r Nretained.00 / nrow(subject.ttest.00) * 100`%) were retained by this criterion.

### Results

```{r 00-plot-raw-responses, warning=FALSE, cache=TRUE, fig.cap="Slider responses by condition. Error bars indicate standard error of the mean. Responses on control items (with ground-truth full or empty locations) are shown as vertical bands, also denoting mean and standard error of the mean."}
df.00.filtered %>%
  ggplot(aes(x=condition_1, y=slider_value)) +
    stat_summary(data=. %>% filter(condition_0 != "filler"), fun=mean,
                 geom="bar") +
    stat_summary(data=. %>% filter(condition_0 != "filler"), fun.data=mean_se,
                 geom="errorbar") +

    stat_summary(aes(x=1, fill=condition_1),
                 data=. %>% filter(condition_1 == "full"), fun.data=mean_se,
                 geom="rect", xmin=0, xmax=Inf, alpha=0.5) +
    stat_summary(aes(x=1, fill=condition_1),
                 data=. %>% filter(condition_1 == "empty"), fun.data=mean_se,
                 geom="rect", xmin=0, xmax=Inf, alpha=0.5) +

    scale_x_discrete("Construction type",
                     labels=c(`0`=example$lbl_Lcons,
                              `1`=example$lbl_Acons)) +
    scale_y_continuous(paste0("Estimated fullness<br/>(*", example$prompt, "*)")) +
    scale_fill_discrete("Filler responses", labels=c("empty"="Empty locations", "full"="Full locations")) +
    theme(axis.text.x = element_markdown(),
          axis.title.y = element_markdown())
```

Figure \@ref(fig:00-plot-raw-responses) shows raw subject slider responses on sentences using the A-construction and L-construction. We see that subjects take the use of the L-construction to mean that the Location is more full (relative to the use of the A-construction).

```{r 00 regression, warning=FALSE, echo=FALSE}
contrasts(df.00.filtered$location_is_subject) = c(-1, 1)

model.00 = xfun::cache_rds({
  brm(slider_value ~ location_is_subject
                   + (location_is_subject | item_id)
                   + (location_is_subject | uniqueid),
      data=df.00.filtered %>%
        filter(condition_0 != "filler") %>%
        mutate(slider_value = (slider_value + 1e-4) / (100 + 2 * 1e-4)),
      family="beta")
})
model.00.tidy = model.00 %>% tidy
model.00.tidy.location_is_subject = model.00.tidy %>%
  filter(term == "location_is_subject1")
```

```{r 00 hdis, echo=FALSE}
# model.00.hdis = model.00 %>%
#   spread_draws(b_location_is_subject1) %>%
#   select(b_location_is_subject1) %>%
#   map(~hdci(.x))
```
We evaluated this effect quantitatively using a Bayesian mixed-effects beta regression. We entered the manipulation *Construction type* (sum-coded; A-construction = -1, L-construction = 1) as a fixed effect in the regression.
We used a maximal random effect structure, including all logically possible by-subject and by-item random slopes and intercepts.
We found a positive effect of *Construction type* (`r print_brms_coef(model.00, "location_is_subject1")`). This shows that comprehenders are sensitive to the meaning difference between the two constructions.

## Production experiment {#experiment-swarm-production-givenness}

```{r load 03 data, include=FALSE}
experiment_names = c("03_production_swarm-givenness")
fillers_id.03 = "fillers/swarm_production-001-twosentences"

df.03 = raw_df %>%
  filter(experiment_id %in% experiment_names,
         trial_type == "survey-multi-choice-ext") %>%

  # TODO double check agent_is_given coding here.
  mutate(agent_is_given = ifelse(condition_0 %in% c(0, 1), condition_0 == 1, NA),
         agent_is_subject = ifelse(condition_0 == "filler", NA, survey_answer == "agent")) %>%

  merge_materials_and_filter

# Demographic information
df.03.demo = raw_df %>%
  # Perform same exclusion as above
  # anti_join(df.04.drops, by=c("uniqueid")) %>%
  filter(experiment_id %in% experiment_names,
         trial_type %in% c("survey-text", "survey-multi-choice"))

filler_example.03 = materials_df %>% filter(materials_id == fillers_id.03, )
```

We next confirmed that speakers were willing to select between the A-construction and L-construction based on given entities in the discourse.
<!-- TODO ref theoretical reason to expect this. -->
<!-- TODO what are the reasons to think *a priori* that realizing as subject might be under different pressures than alternation of postverbal content as in the dative alternation? -->

### Procedure

```{r 03-vignette, fig.cap="Sample trial interface from the production experiment."}
knitr::include_graphics("figs/vignette_03.png")
```

This experiment used the complete two-sentence stimuli described in \@ref(stimuli). Subjects performed forced-choice judgments over pairs of these stimuli which differed only in their second sentence. Thus each trial held a *Given entity* fixed while contrasting the two construction types. The factor of *Given entity* was manipulated within-subject and within-item, and assignment was pseudo-randomly counterbalanced among subjects.
Each trial presented these two options (in random order) and asked subjects to judge which was the more "natural" of the pair. An example trial is shown in Figure \@ref(fig:03-vignette).

The experiment also contained `r nrow(df.03 %>% filter(condition_0 == "filler") %>% select(item_id) %>% distinct())` filler items. Each filler item likewise contrasted pairs of two-sentence stimuli. The items of each pair shared a first sentence but differed in their second sentence, containing either a natural or less natural completion. The less natural items were often grammatical, but contained awkward or rare word orders, or used non-standard multi-word expressions. These filler items are presented in \@ref(tbl prod fillers).

### Participants

```{r, results='asis'}
# TODO exclude native speakers
demo_chunk(df.03.demo)
```

### Exclusion criteria {#swarm-2afc-exclusion-criteria}

```{r 03 exclusion, echo=FALSE}
# Exclude subjects whose success rate is lower than this threshold
exclude_threshold.03 = 0.7

# Early phases of data collection revealed that some fillers don't illustrate
# the contrast appropriately. (Subjects otherwise performing well missed these
# items.) We'll drop them from the exclusion routine.
drop_fillers.03 = c(6)

subject.exclude.03 = df.03 %>%
  filter(
    condition_0 == "filler",
    !(item_id %in% drop_fillers.03)) %>%
  mutate(success=survey_answer == "good") %>%
  group_by(uniqueid) %>%
    summarise(success_rate=mean(success)) %>%
  mutate(exclude=success_rate < exclude_threshold.03)
Nretained.03 = nrow(subject.exclude.03 %>% filter(!exclude))

df.03.filtered = df.03 %>%
  left_join(subject.exclude.03, by=c("uniqueid")) %>%
  filter(!exclude)
```

We used subjects' behavior on filler items to determine an exclusion criterion. We retained only subjects who chose the more natural completion in `r round(exclude_threshold.03 * 100)`% of filler trials.
`r Nretained.03` subjects (`r Nretained.03 / nrow(subject.exclude.03) * 100`%) were retained by this criterion.

### Results

Figure \@ref(fig:03-raw-response-plot) shows the proportion of subjects who chose completion sentences using the A-construction as a function of the *Given entity* in the first sentence. We see that subjects prefer to use the A-construction when the Agent/Theme is established as given by the previous sentence.

```{r 03-raw-response-plot, warning=FALSE, fig.cap="Proportion of subjects choosing A-construction as a function of the *Given entity* manipulation. Horizontal line indicates 50% (response due to random guessing). Error bars are 95% confidence intervals under the Clopper--Pearson test."}
df.03.filtered %>%
  filter(condition_0 != "filler") %>%
  ggplot(aes(x=condition_0, y=as.numeric(agent_is_subject))) +
    stat_summary(geom="bar", fun=mean, position="dodge") +
    stat_summary(geom="errorbar", fun.data=reskew_binom_test) +

    # TODO error bar

    # reference line at y = 0.5
    geom_hline(yintercept=0.5) +

    scale_x_discrete(name="Given entity",
                     labels=c(`0`=example$lbl_Lgiv,
                              `1`=example$lbl_Agiv)) +
    scale_y_continuous(name=paste0("Proportion choosing A-construction",
                                   "<br/>(*", example$Acons, "*)"),
                       limits=c(0, 1)) +

    theme(axis.text.x = element_markdown(),
          axis.title.y = element_markdown())
```

```{r 03 regression, echo=FALSE}
model.03 = xfun::cache_rds({
  glmer(agent_is_subject ~ agent_is_given
                         + (agent_is_given | item_id)
                         + (agent_is_given | uniqueid),
        family="binomial",
        data=df.03.filtered %>%
          filter(condition_0 != "filler"),
        control = glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun=2e5)),
        contrasts=list(agent_is_given=c(-1, 1)))
})
model.03.print = apa_print(model.03)
```

We confirmed this effect quantitatively with a mixed-effects logistic regression, predicting the binary response (L-construction = 0; A-construction = 1) as a function of the *Given entity* (sum-coded; Location given = -1, Agent/Theme given = 1). The regression included the maximal random effects structure by subject and item.
We found a significant effect of the manipulation of *Given entity* (`r model.03.print$full_result$agent_is_given1`).

This experiment confirmed that speakers' construction choice is sensitive to discourse structure: specifically, that speakers change their choice of construction in order to place given entities in subject position.

## Critical explaining-away experiment {#experiment-swarm-comprehension-full}

```{r load 04 data}
experiment_names = c("04_comprehension_swarm-full", "09_comprehension_swarm-full-nonalternating-control")
num_fillers.04 = 12

df.04 = raw_df %>%
  filter(experiment_id %in% experiment_names,
         trial_type == "html-slider-response-with-copout",
         # Drop nonalternating control trials.
         condition_1 != 2) %>%

  # TODO double double check condition_1 codes here, including consistency between two experiment runs
  mutate(location_is_subject = ifelse(condition_1 %in% c(0, 1), condition_1 == 0, NA),
         agent_is_given = ifelse(condition_0 %in% c(0, 1), condition_0 == 0, NA),
         condition = ifelse(condition_0 == "filler", paste(condition_0, condition_1),
                            paste0("c", ifelse(location_is_subject, "L", "A"),
                                   "g", ifelse(agent_is_given, "A", "L")))) %>%

  merge_materials_and_filter

# Drop subjects with the wrong number of filler items.
# TODO what is causing this problem in the first place?
df.04.drops = df.04 %>%
  filter(condition_0 == "filler") %>%
  group_by(uniqueid) %>%
    summarise(n=n()) %>%
  filter(n != num_fillers.04)
df.04 = df.04 %>% anti_join(df.04.drops, by=c("uniqueid"))

# Demographic information
df.04.demo = raw_df %>%
  # Perform same exclusion as above
  anti_join(df.04.drops, by=c("uniqueid")) %>%
  filter(experiment_id %in% experiment_names,
         trial_type %in% c("survey-text", "survey-multi-choice"))
```

Section \@ref(experiment-swarm-comprehension-basic) confirmed that English speakers infer a meaning contrast between the A-construction and L-construction. Section \@ref(experiment-swarm-production-givenness) further confirmed that English speakers know to select between the A-construction and L-construction in response to features of the discourse. In this final experiment, we test whether English speakers use this knowledge about the multiple pressures faced by the speaker in selecting a construction to adjust their inference about the speaker's intended meaning.

TODO: Link with walkthrough from introduction, describing expected data pattern if explaining-away were to happen.

### Procedure

```{r 04-vignette, fig.cap="Sample trial interface for the full comprehension experiment."}
knitr::include_graphics("./figs/vignette_04.png")
```

`r describe_stimuli_two_sentence()`
We used the same slider interface and filler items as described in Section \@ref(experiment-swarm-comprehension-basic).
An example trial is shown in Figure \@ref(fig:04-vignette).

### Participants

```{r, results='asis'}
# TODO exclude native speakers
# TODO figure out the person who wrote 1983
demo_chunk(df.04.demo)
```

### Exclusion criteria

We used the same exclusion criteria as in the basic comprehension experiment, described in Section \@ref(experiment-swarm-comprehension-basic).

```{r 04 exclusion, warning=FALSE}
subject.ttest.04 = run_slider_exclusions(
  df.04 %>% filter(condition_0 == "filler"), condition_1, full, empty)
Nretained.04 = nrow(subject.ttest.04 %>% filter(!exclude))

df.04.filtered = df.04 %>%
  left_join(subject.ttest.04 %>% select(uniqueid, exclude), by=c("uniqueid")) %>%
  filter(exclude == FALSE)
```

`r Nretained.04` subjects (`r Nretained.04 / nrow(subject.ttest.04) * 100`%) were retained by this criterion.

### Results

```{r 04-plot-raw-responses, warning=FALSE, echo=FALSE, fig.cap="Slider responses by condition. Error bars indicate standard error of the mean. Responses on control items (with ground-truth full or empty locations) are shown as horizontal bands, also denoting mean and standard error of the mean."}
df.04.filtered %>%
  ggplot(aes(x=location_is_subject, fill=agent_is_given, y=slider_value)) +
    stat_summary(data=. %>% filter(condition_0 != "filler"), fun=mean,
                 geom="bar", position="dodge") +
    stat_summary(data=. %>% filter(condition_0 != "filler"), fun.data=mean_se,
                 geom="errorbar", position="dodge") +

    stat_summary(aes(x=1, fill=NULL, color=condition_1),
                 data=. %>% filter(condition_1 == "full"), fun.data=mean_se,
                 geom="rect", xmin=0, xmax=Inf, alpha=0.5) +
    stat_summary(aes(x=1, fill=NULL, color=condition_1),
                 data=. %>% filter(condition_1 == "empty"), fun.data=mean_se,
                 geom="rect", xmin=0, xmax=Inf, alpha=0.5) +

    scale_fill_discrete("Given entity",
                        labels=c(`TRUE`=example$lbl_Agiv,
                                 `FALSE`=example$lbl_Lgiv)) +
    scale_x_discrete("Construction type", labels=c(`TRUE`=example$lbl_Lcons, `FALSE`=example$lbl_Acons)) +
    scale_y_continuous(paste0("Estimated fullness<br/>(*", example$prompt, "*)")) +
    scale_color_brewer("Filler responses", palette="Set3",
                         labels=c("empty"="Empty locations", "full"="Full locations")) +

    theme(axis.title.y = element_markdown(),
          axis.text.x = element_markdown(),
          legend.text = element_markdown())
```

Figure \@ref(fig:04-plot-raw-responses) shows the raw responses of subjects across the four experimental conditions.
We evaluated the effect of our experimental manipulation using a Bayesian mixed-effects beta regression. We entered the manipulations *Given entity* (sum-coded; Location = -1, Agent/Theme = 1) and *Construction type* (sum-coded; A-construction = -1, L-construction = 1) as fixed effects in the regression, along with the interaction of the two variables. (Note that both manipulations are coded such that we should expect a higher rating on the fullness measure for positive values of the factor.)
We used a maximal random effect structure, including all logically possible by-subject and by-item random slopes and intercepts.

```{r 04 regression, warning=FALSE, echo=FALSE}
contrasts(df.04.filtered$location_is_subject) = c(-1, 1)
contrasts(df.04.filtered$agent_is_given) = c(-1, 1)

model.04 = xfun::cache_rds({
  brm(slider_value ~ location_is_subject * agent_is_given
                   + (location_is_subject * agent_is_given | item_id)
                   + (location_is_subject * agent_is_given | uniqueid),
      data=df.04.filtered %>%
        filter(condition_0 != "filler") %>%
        mutate(slider_value = (slider_value + 1e-4) / (100 + 2 * 1e-4)),
      family="beta")
})
```
```{r 04 HDIs, include=FALSE}
model.04.fixef_draws = model.04 %>%
  spread_draws(b_location_is_subject1, b_agent_is_given1)
model.04.hdis = model.04.fixef_draws %>%
  select(-starts_with(".")) %>% map(~{hdi(.)})
model.04.hdis
```

We again found a positive effect of *Construction type* (`r print_brms_coef(model.04, "location_is_subject1")`) but found no clear positive effect of *Given entity*, with a 95% credible interval on its coefficient including zero and negative values (`r print_brms_coef(model.04, "agent_is_given1")`). The interaction of the two factors also had no clear effect (`r print_brms_coef(model.04, "location_is_subject1:agent_is_given1")`).

TODO: calculate pMCMC p-value

\newpage

# (APPENDIX) Appendix {-}

# Control experiments
## Single-sentence acceptability rating experiment {#experiment-swarm-acceptability}

This control experiment confirmed that our uses of the A-construction and L-construction were deemed acceptable by English speakers.

```{r load 02 data, echo=FALSE}
experiment_name = "02_acceptability_swarm"

df.02 = raw_df %>%
  filter(experiment_id == experiment_name,
         trial_type == "html-slider-response-with-copout") %>%
  mutate(agent_is_subject = ifelse(condition_1 %in% c(0, 1), condition_1 == 1, NA)) %>%

  merge_materials_and_filter

# Demographic information
df.02.demo = raw_df %>%
  filter(experiment_id == experiment_name,
         trial_type %in% c("survey-text", "survey-multi-choice"))
```

### Procedure

Subjects were presented with sentences drawn from our materials using either the A-construction or the L-construction, and asked to rate how "natural" each sentence was in each trial on a 7-point Likert scale. Each subject completed several practice trials prior to the experiment designed to calibrate this notion of a "natural" sentence.
We varied construction type both within-subject and within-item, and the assignment was pseudo-randomly counterbalanced across subjects.
Subjects were also presented with grammatical and ungrammatical filler sentences. Filler and experimental trials were pseudo-randomly shuffled.

### Participants

```{r, results='asis'}
# TODO exclude native speakers
demo_chunk(df.02.demo)
```

### Exclusion criteria

```{r 02 exclusion}
subject.ttest.02 = run_slider_exclusions(
  df.02 %>% filter(condition_0 == "filler"), condition_1, good, bad)
Nretained.02 = nrow(subject.ttest.02 %>% filter(!exclude))

df.02.filtered = df.02 %>%
  left_join(subject.ttest.02 %>% select(uniqueid, exclude), by=c("uniqueid")) %>%
  filter(exclude == FALSE)
```

We used subjects' performance on filler items to design an exclusion criterion. For each subject, we collect their Likert responses on filler items, which have ground-truth "acceptable" and "unacceptable" labels. We then perform a two-sample one-tailed $t$-test between these groups. We retain only those subjects for which we find that $\text{rating}_\text{acceptable} > \text{rating}_\text{unacceptable}$ with one-tailed $p < 0.1$.

`r Nretained.02` subjects (`r Nretained.02 / nrow(subject.ttest.02) * 100`%) were retained by this criterion.

### Results

We evaluated the effect of our experimental manipulation using a mixed-effects linear regression model, comparing acceptability ratings on each construction (A-construction or L-construction) to ungrammatical items. The fixed effect of interest was this *Contrast* factor (treatment-coded; ungrammatical items = 0, A-construction = 1, L-construction = 1).
<!-- We included as sum-coded covariates *Countability* (whether the agent/theme was a mass or count noun) and *Animacy* (whether the agent/theme expressed an animate participant). Finally, we included interaction terms between these control variables and the *Contrast* factor. -->
We used a maximal random effect structure, including all logically possible by-subject and by-item random slopes and intercepts.

```{r 02 regression, echo=FALSE}
# TODO acceptability regression incorporates animacy and countability, but 1) these aren't defined for fillers and 2) filler materials ID didn't get recorded correctly for early experiments.
# #1 is easy to fix. #2 we need to recover the correct materials for fillers
# For now, going to leave out those control variables.

model.02 = xfun::cache_rds({do_acceptability_regression(df.02.filtered, condition_1, c(0, 1), "bad")})
model.02.print = papaja::apa_print(model.02$model)
```

The regression shows a significant main effect relative to ungrammatical English items for both constructions (A-construction: `r model.02.print$statistic$contrast1`; L-construction: `r model.02.print$statistic$contrast0`).

```{r, fig.cap="Acceptability responses for L-construction, A-construction, and ungrammatical filler items, after controlling for by-item and by-subject random effects."}
# # TODO this is a very unintuitive graph, because it reveals how our regression assumptions are violated :)
# # After controlling for estimated subject/item effets, some responses are higher than 7!
# model.02$df_controlled %>%
#   ggplot(aes(x=condition_1, y=slider_value_controlled)) +
#   geom_boxplot() +
#   geom_quasirandom(alpha=0.3) +
#   scale_y_continuous(name="Acceptability rating", breaks=c(1, 2, 3, 4, 5, 6, 7)) +
#   scale_x_discrete(name="Condition", labels=c(`0`="L-construction", `1`="A-construction", "bad"="Ungrammatical fillers"))
```

```{r 02 raw responses by condition, out.width="75%", fig.cap="Acceptability ratings by condition, compared to grammatical and ungrammatical filler items. Error bars indicate standard error."}
df.02.filtered %>%
  ggplot(aes(x=condition_1, y=slider_value)) +
    stat_summary(fun=mean, geom="bar") +
    stat_summary(fun.data=mean_se, geom="errorbar") +
    scale_y_continuous(name="Acceptability rating", breaks=c(1, 2, 3, 4, 5, 6, 7)) +
    scale_x_discrete(name="Condition",
                     labels=c(`0`=example$lbl_Lcons,
                              `1`=example$lbl_Acons,
                              "bad"="Ungrammatical fillers",
                              "good"="Grammatical fillers")) +
    theme(axis.text.x=element_markdown())
```

```{r 02 raw responses by condition and item, fig.cap="Acceptability ratings by condition and item. Error bars indicate standard error."}
plot_acceptability_by_item(df.02.filtered, condition_1,
                           condition_labels=c(`0`="L-construction", `1`="A-construction"))
```

## Two-sentence acceptability experiment {#experiment-swarm-acceptability-two}

Our full experiments use stimuli containing two sentences each. The first sentence establishes an entity as a discourse referent, and the second sentence uses the A-construction or L-construction with that entity expressed anaphorically. We next tested whether these two-sentence stimuli were natural for English speakers.

```{r load 08 data}
experiment_name = "08_acceptability_swarm-withprefix"

df.08 = raw_df %>%
  filter(experiment_id == experiment_name,
         trial_type == "html-slider-response-with-copout") %>%
  mutate(agent_is_subject = ifelse(condition_1 %in% c(0, 1), condition_1 == 1, NA),
         agent_is_given = ifelse(condition_0 %in% c(0, 1), condition_0 == 0, NA),
         condition = ifelse(condition_0 == "filler", paste(condition_0, condition_1),
                            paste0("c", ifelse(agent_is_subject, "A", "L"),
                                   "g", ifelse(agent_is_given, "A", "L")))) %>%

  merge_materials_and_filter

# Demographic information
df.08.demo = raw_df %>%
  filter(experiment_id == experiment_name,
         trial_type %in% c("survey-text", "survey-multi-choice"))
```

### Procedure

`r describe_stimuli_two_sentence()` Subjects were asked to rate how "natural" each passage was in each trial on a 7-point Likert scale. Each subject completed several practice trials prior to the experiment designed to calibrate this notion of a "natural" passage.
Subjects were also presented with grammatical and ungrammatical filler sentences. Filler and experimental trials were pseudo-randomly shuffled.

### Participants

```{r, results='asis'}
# TODO exclude native speakers
demo_chunk(df.08.demo)
```

### Exclusion criteria

```{r 08 exclusion}
subject.ttest.08 = run_slider_exclusions(
  df.08 %>% filter(condition_0 == "filler"), condition_1, good, bad)
Nretained.08 = nrow(subject.ttest.08 %>% filter(!exclude))

df.08.filtered = df.08 %>%
  left_join(subject.ttest.08 %>% select(uniqueid, exclude), by=c("uniqueid")) %>%
  filter(exclude == FALSE)
```

We used subjects' performance on filler items to design an exclusion criterion. For each subject, we collect their Likert responses on filler items, which have ground-truth "acceptable" and "unacceptable" labels. We then perform a two-sample one-tailed $t$-test between these groups. We retain only those subjects for which we find that $\text{rating}_\text{acceptable} > \text{rating}_\text{unacceptable}$ with one-tailed $p < 0.1$.
`r Nretained.08` subjects (`r Nretained.08 / nrow(subject.ttest.08) * 100`%) were retained by this criterion.

### Results

```{r 08 regression, echo=FALSE}
model.08 = xfun::cache_rds({
  do_acceptability_regression(df.08.filtered, condition, c("cAgA", "cAgL", "cLgL", "cLgA"), "filler bad")
})

model.08.print = papaja::apa_print(model.08$model)
```

```{r 08 boxplots, fig.cap="Acceptability responses for two-sentence stimuli and ungrammatical filler items, after controlling for by-item and by-subject random effects."}
# # TODO this is a very unintuitive graph, because it reveals how our regression assumptions are violated :)
# # After controlling for estimated subject/item effects, some responses are higher than 7!
# model.08$df_controlled %>%
#   ggplot(aes(x=condition, y=slider_value_controlled)) +
#   geom_boxplot() +
#   geom_quasirandom(alpha=0.3) +
#   scale_y_continuous(name="Acceptability rating", breaks=c(1, 2, 3, 4, 5, 6, 7))
#
#   # TODO make this scale
#   # scale_x_discrete(name="Condition", labels=c(`0`="L-construction", `1`="A-construction", "bad"="Ungrammatical fillers"))
```

```{r 08 raw responses by condition, fig.cap="Acceptability ratings by condition, compared to grammatical and ungrammatical filler items. Error bars indicate standard error."}
df.08.filtered %>%
  ggplot(aes(x=condition, y=slider_value)) +
    stat_summary(fun=mean, geom="bar") +
    stat_summary(fun.data=mean_se, geom="errorbar") +
    scale_y_continuous(name="Acceptability rating", breaks=c(1, 2, 3, 4, 5, 6, 7)) +
    scale_x_discrete(name="Condition", labels=c("cLgL"="cLgL", "cLgA"="cLgA",
                                                "cAgA"="cAgA", "cAgL"="cAgL",
                                                "bad"="Ungrammatical fillers",
                                                "good"="Grammatical fillers"))
```

```{r 08 raw responses by condition and item, fig.cap="Acceptability ratings by condition and item. Error bars indicate standard error."}
plot_acceptability_by_item(df.08.filtered, condition,
                           condition_labels=c("cLgL"="cLgL", "cLgA"="cLgA",
                                              "cAgA"="cAgA", "cAgL"="cAgL"))
```

## Controlling for the effect of the prefix sentence {#experiment-swarm-comprehension-control}

Our full comprehension experiment manipulated the *Given entity* by changing the content of the first sentence to establish either the Agent/Theme or Location as an available discourse referent. This change of content may also have unintended effects on the critical measure, creating noise in the data which may have washed out any possible signal of the explaining-away effect. In this control experiment, we wish to verify that the first sentence's content has little effect on the interpretation of the fullness of the Location.

### Procedure

| Given entity $\rightarrow$ | Stimulus |
| -------------------------- | -------- |
| Agent/Theme | The town is trying to clean up the storm debris. It is collecting in the river. |
| Location | The town is trying to clean up the river. Debris is collecting there. |

Table: (#tab:control-nonalternating-item) Sample item for prefix sentence control experiment.

We designed a variant of the two-sentence experimental materials, changing the second sentence to express a relation between the Agent/Theme and Location using a Verb which des not alternate between the A-construction and L-construction. An example item is shown in Table \@ref(tab:control-nonalternating-item), rendered in two conditions which manipulate the *Given entity*.
<!-- TODO talk about which predicates these are -->

```{r load 09 data}
experiment_names = c("09_comprehension_swarm-full-nonalternating-control")

df.09c = raw_df %>%
  filter(experiment_id %in% experiment_names,
         trial_type == "html-slider-response-with-copout",
         # Retain only fillers and nonalternating control trials.
         condition_0 == "filler" | condition_1 == 2) %>%

  # TODO double double check condition_1 codes here, including consistency between two experiment runs
  mutate(agent_is_given = ifelse(condition_0 %in% c(0, 1), condition_0 == 0, NA)) %>%

  merge_materials_and_filter %>%

  # pick an easier variable name
  rename(nonalternating_predicate=`non alternating given A`)

# Demographic information
df.09c.demo = raw_df %>%
  # Perform same exclusion as above
  # anti_join(df.04.drops, by=c("uniqueid")) %>%
  filter(experiment_id %in% experiment_names,
         trial_type %in% c("survey-text", "survey-multi-choice"))
```

### Participants

```{r, results='asis'}
# TODO exclude native speakers
demo_chunk(df.09c.demo)
```

### Exclusion criteria

We used the same exclusion criteria as in \@ref(experiment-swarm-comprehension-basic).

```{r 09c exclusion, warning=FALSE}
subject.ttest.09c = run_slider_exclusions(
  df.09c %>% filter(condition_0 == "filler"), condition_1, full, empty)
Nretained.09c = nrow(subject.ttest.09c %>% filter(!exclude))

df.09c.filtered = df.09c %>%
  left_join(subject.ttest.09c %>% select(uniqueid, exclude), by=c("uniqueid")) %>%
  filter(exclude == FALSE)
```

`r Nretained.09c` subjects (`r Nretained.09c / nrow(subject.ttest.09c) * 100`%) were retained by this criterion.

### Results

We evaluated the effect of our experimental manipulation using a Bayesian mixed-effects beta regression. We entered the manipulation *Given entity* (sum-coded; Location = -1, Agent/Theme = 1) as a fixed effect in the regression.
We used a maximal random effect structure, including all logically possible by-subject and by-item random slopes and intercepts.

```{r 09c regression, echo=FALSE}
contrasts(df.09c.filtered$agent_is_given) = c(-1, 1)

model.09c = xfun::cache_rds({
  brm(slider_value ~ agent_is_given
                   + (agent_is_given | nonalternating_predicate/item_id)
                   + (agent_is_given | uniqueid),
      data=df.09c.filtered %>%
        filter(condition_0 != "filler") %>%
        mutate(slider_value = (slider_value + 1e-4) / (100 + 2 * 1e-4)),
      family="beta")
})
```

```{r 09c HDIs, include=FALSE}
model.09c.fixef_draws = model.09c %>%
  spread_draws(b_agent_is_given1, `r_nonalternating_predicate:item_id.+Intercept]`, regex=TRUE)
model.09c.hdis = model.09c.fixef_draws %>%
  select(-starts_with(".")) %>% map(~{hdci(.)})
model.09c.hdis
```

# Materials
## Comprehension materials

```{r tbl comp 00 materials}
# TODO render full sentences
df.00 %>%
  filter(materials_id == "swarm-003-drops") %>%
  select(materials_id, item_id) %>%
  distinct() %>%
  left_join(materials_df, by=c("materials_id", "item_id"="id")) %>%
  select(item_id, A, V, P, `L det`, L) %>%
  mutate(`L det`=ifelse(`L det` == "%PERSON1_POSS%", "his/her", `L det`),
         V=paste0(V, "ing")) %>%
  arrange(V) %>%
  select(-item_id) %>%
  knitr::kable("pipe",
               col.names=c("Agent/Theme", "Verb", "Preposition", "Location determiner", "Location"),
               caption="Experimental items used to generate A-construction and L-construction sentences in comprehension experiments.")
```

## Comprehension fillers

```{r tbl comp filler materials}
materials_df %>%
  filter(materials_id == fillers_id.00) %>%
  select(id, rating, sentence, prompt) %>%
  mutate(rating=R.utils::capitalize(rating)) %>%
  arrange(rating, id) %>%
  select(-id) %>%
  knitr::kable("pipe",
               col.names=R.utils::capitalize(colnames(.)),
               caption="Filler items used in comprehension experiments.")
```

## Production fillers

```{r tbl prod fillers}
# TODO colnames, manipulation name
materials_df %>%
  filter(materials_id == fillers_id.03) %>%
  select(id, manipulation, prefix, good_completion, bad_completion) %>%
  mutate(prefix=paste0(prefix, ".")) %>%
  arrange(manipulation, id) %>%
  select(-id) %>%
  knitr::kable("pipe",
               col.names=R.utils::capitalize(colnames(.)),
               caption="Filler items used in production experiments.")
```

# Raw response data

```{r 02 slider visualization, warning=FALSE, cache=TRUE, fig.cap="Visualization of slider response behavior in the single-sentence acceptability experiment by subject. Boxplots show response behavior on filler items; we expect filler responses to lie at the extremes of the scale. Swarmplots show responses on experimental items."}
df.02 %>%

  # Join in filler t-test results
  full_join(select(subject.ttest.02, uniqueid, exclude, p.value) %>% rename(filler.p.value=p.value),
            by=c("uniqueid")) %>%

  mutate(
    # Reorder subjects based on filler test p-value
    uniqueid=fct_reorder(uniqueid, filler.p.value)) %>%

  ggplot(aes(x=uniqueid, y=slider_value, color=condition_1)) +

    geom_quasirandom(groupOnX=TRUE, alpha=0.4, data=. %>% filter(condition_1 %in% c(0, 1))) +
    scale_color_discrete(name="Stimulus condition",
                         labels=c(`0`="L-construction", `1`="A-construction",
                                  "bad"="Ungrammatical filler", "good"="Grammatical filler")) +

    # Plot boxplots for fillers
    geom_boxplot(alpha=0.2, data=. %>% filter(condition_1 == "good")) +
    geom_boxplot(alpha=0.2, data=. %>% filter(condition_1 == "bad")) +

    # Shade based on exclusion
    geom_rect(data=. %>% group_by(uniqueid) %>% filter(row_number() == 1),
              aes(xmin = as.numeric(uniqueid) - 0.5, xmax = as.numeric(uniqueid) + 0.5,
                  ymin = 1, ymax = 7, fill = exclude),
              alpha = 0.1, color=NA) +
    scale_fill_discrete(name="Excluded?", labels=c(`FALSE`="No", `TRUE`="Yes")) +

    scale_x_discrete(name="Subject", labels=1:nrow(subject.ttest.02)) +
    scale_y_continuous(name="Acceptability rating", breaks=c(1, 2, 3, 4, 5, 6, 7))
```

```{r 08 slider visualization, warning=FALSE, cache=TRUE, fig.cap="Visualization of slider response behavior in the two-sentence acceptability experiment by subject. Boxplots show response behavior on filler items; we expect filler responses to lie at the extremes of the scale. Swarmplots show responses on experimental items."}
df.08 %>%

  # Join in filler t-test results
  full_join(select(subject.ttest.08, uniqueid, exclude, p.value) %>% rename(filler.p.value=p.value),
            by=c("uniqueid")) %>%

  mutate(
    # Reorder subjects based on filler test p-value
    uniqueid=fct_reorder(uniqueid, filler.p.value),

    plot_group=ifelse(condition_0 == "filler", condition_1,
                      "exp")) %>%

  ggplot(aes(x=uniqueid, y=slider_value, color=plot_group)) +

    geom_quasirandom(groupOnX=TRUE, alpha=0.4, data=. %>% filter(plot_group == "exp")) +
    scale_color_discrete(name="Stimulus type",
                         labels=c("exp"="Experimental item",
                                  "bad"="Ungrammatical filler", "good"="Grammatical filler")) +

    # Plot boxplots for fillers
    geom_boxplot(alpha=0.2, data=. %>% filter(condition_1 == "good")) +
    geom_boxplot(alpha=0.2, data=. %>% filter(condition_1 == "bad")) +

    # Shade based on exclusion
    geom_rect(data=. %>% group_by(uniqueid) %>% filter(row_number() == 1),
              aes(xmin = as.numeric(uniqueid) - 0.5, xmax = as.numeric(uniqueid) + 0.5,
                  ymin = 1, ymax = 7, fill = exclude),
              alpha = 0.1, color=NA) +
    scale_fill_discrete(name="Excluded?", labels=c(`FALSE`="No", `TRUE`="Yes")) +

    scale_x_discrete(name="Subject", labels=1:nrow(subject.ttest.08)) +
    scale_y_continuous(name="Acceptability rating", breaks=c(1, 2, 3, 4, 5, 6, 7))
```

```{r 00 slider visualization, warning=FALSE, cache=TRUE, echo=FALSE, fig.cap="Visualization of slider response behavior in the basic comprehension experiment by subject. Boxplots show response behavior on filler items; we expect filler responses to lie at the extremes of the scale. Swarmplots show responses on experimental items.", cache=TRUE}
df.00 %>%

  # Join in filler t-test results
  left_join(select(subject.ttest.00, uniqueid, exclude, p.value) %>% rename(filler.p.value=p.value),
            by=c("uniqueid")) %>%

  mutate(
    # Reorder subjects based on p-value of filler t-test
    uniqueid=fct_reorder(uniqueid, filler.p.value),

    plot_group=ifelse(condition_0 == "filler", condition_1,
                      "exp")) %>%

  ggplot(aes(x=uniqueid, y=slider_value, color=plot_group)) +

    geom_quasirandom(groupOnX=TRUE, alpha=0.4, data=. %>% filter(plot_group == "exp")) +
    scale_color_discrete(name="Stimulus type",
                         labels=c("exp"="Experimental item",
                                  "empty"="Empty filler", "full"="Full filler")) +

    # Plot boxplots for fillers
    geom_boxplot(alpha=0.2, data=. %>% filter(condition_1 == "full")) +
    geom_boxplot(alpha=0.2, data=. %>% filter(condition_1 == "empty")) +

    # Shade based on exclusion
    geom_rect(data=. %>% group_by(uniqueid) %>% filter(row_number() == 1),
              aes(xmin = as.numeric(uniqueid) - 0.5, xmax = as.numeric(uniqueid) + 0.5,
                  ymin = 0, ymax = 100, fill = exclude),
              alpha = 0.1, color=NA) +
    scale_fill_discrete(name="Excluded?", labels=c(`FALSE`="No", `TRUE`="Yes")) +

    scale_x_discrete(name="Subject", labels=1:nrow(subject.ttest.00)) +
    scale_y_continuous(name="Fullness rating") +
    theme()
```

```{r 04 slider visualization, warning=FALSE, cache=TRUE, echo=FALSE, fig.cap="Visualization of slider response behavior in the full comprehension experiment by subject. Boxplots show response behavior on filler items; we expect filler responses to lie at the extremes of the scale. Swarmplots show responses on experimental items.", cache=TRUE}
df.04 %>%

  # Join in filler t-test results
  full_join(select(subject.ttest.04, uniqueid, exclude, p.value) %>% rename(filler.p.value=p.value),
            by=c("uniqueid")) %>%

  mutate(
    # Reorder subjects based on filler test p-value
    uniqueid=fct_reorder(uniqueid, filler.p.value),

    plot_group=ifelse(condition_0 == "filler", condition_1,
                      "exp")) %>%

  ggplot(aes(x=uniqueid, y=slider_value, color=plot_group)) +

    geom_quasirandom(groupOnX=TRUE, alpha=0.4, data=. %>% filter(plot_group == "exp")) +
    scale_color_discrete(name="Stimulus type",
                         labels=c("exp"="Experimental item",
                                  "empty"="Empty filler", "full"="Full filler")) +

    # Plot boxplots for fillers
    geom_boxplot(alpha=0.2, data=. %>% filter(condition_1 == "full")) +
    geom_boxplot(alpha=0.2, data=. %>% filter(condition_1 == "empty")) +

    # Shade based on exclusion
    geom_rect(data=. %>% group_by(uniqueid) %>% filter(row_number() == 1),
              aes(xmin = as.numeric(uniqueid) - 0.5, xmax = as.numeric(uniqueid) + 0.5,
                  ymin = 0, ymax = 100, fill = exclude),
              alpha = 0.1, color=NA) +
    scale_fill_discrete(name="Excluded?", labels=c(`FALSE`="No", `TRUE`="Yes")) +

    scale_x_discrete(name="Subject", labels=1:nrow(subject.ttest.04)) +
    scale_y_continuous(name="Fullness rating")
```

```{r 09c slider visualization, warning=FALSE, cache=TRUE, echo=FALSE, fig.cap="Visualization of slider response behavior in the control comprehension experiment by subject. Boxplots show response behavior on filler items; we expect filler responses to lie at the extremes of the scale. Swarmplots show responses on experimental items.", cache=TRUE}
df.09c %>%

  # Join in filler t-test results
  full_join(select(subject.ttest.09c, uniqueid, exclude, p.value) %>% rename(filler.p.value=p.value),
            by=c("uniqueid")) %>%

  mutate(
    # Reorder subjects based on filler test p-value
    uniqueid=fct_reorder(uniqueid, filler.p.value),

    plot_group=ifelse(condition_0 == "filler", condition_1,
                      "exp")) %>%

  ggplot(aes(x=uniqueid, y=slider_value, color=plot_group)) +

    geom_quasirandom(groupOnX=TRUE, alpha=0.4, data=. %>% filter(plot_group == "exp")) +
    scale_color_discrete(name="Stimulus type",
                         labels=c("exp"="Experimental item",
                                  "empty"="Empty filler", "full"="Full filler")) +

    # Plot boxplots for fillers
    geom_boxplot(alpha=0.2, data=. %>% filter(condition_1 == "full")) +
    geom_boxplot(alpha=0.2, data=. %>% filter(condition_1 == "empty")) +

    # Shade based on exclusion
    geom_rect(data=. %>% group_by(uniqueid) %>% filter(row_number() == 1),
              aes(xmin = as.numeric(uniqueid) - 0.5, xmax = as.numeric(uniqueid) + 0.5,
                  ymin = 0, ymax = 100, fill = exclude),
              alpha = 0.1, color=NA) +
    scale_fill_discrete(name="Excluded?", labels=c(`FALSE`="No", `TRUE`="Yes")) +

    scale_x_discrete(name="Subject", labels=1:nrow(subject.ttest.09c)) +
    scale_y_continuous(name="Fullness rating")
```

# References

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in}
\setlength{\leftskip}{0.125in}
\noindent
